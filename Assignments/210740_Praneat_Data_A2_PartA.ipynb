{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A\n",
    "\n",
    "**Name**: Praneat Data                               </br>\n",
    "**Roll No.**: 210740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Stochastic Maze problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticMazeEnv(gym.Env):\n",
    "    '''\n",
    "    StochasticMazeEnv represents the Gym Environment for the Stochastic Maze environment\n",
    "    States : [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    Actions : [\"Left\":0, \"Up\":1, \"Right\":2, \"Down\":3]\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=12,no_actions=4):\n",
    "        '''\n",
    "        Constructor for the StochasticMazeEnv class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 12\n",
    "            no_actions : The no. of possible actions which is 4\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.actions_dict = {\"L\":0, \"U\":1, \"R\":2, \"D\":3}\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        index = np.random.choice(3,1,p=[0.8,0.1,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StochasticMazeEnv()\n",
    "env.reset()\n",
    "num_states = env.nS\n",
    "num_actions = env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases for checking the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 11\n",
      "Final Reward: 0.9\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    rand_action = np.random.choice(4,1)[0]  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(rand_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", rand_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random policy takes large number of steps to reach some terminal state which should be much higher than the number of the steps taken by a all 'Right' policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Right Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 6\n",
      "Final Reward: 0.95\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    right_action = 2  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(right_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The right policy most of the time reaches the goal state in just 3 steps which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Find an optimal policy to navigate the given environment using Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated in 7 policy iterations\n",
      "Optimal Policy:[2 2 2 0 1 0 1 0 1 0 1 0] \n",
      "Value Function:[  7.13978218   8.25567002   9.41617975  10.           6.25688192\n",
      "   0.           6.34137299 -10.           5.40792881   4.73622948\n",
      "   5.25657209   3.0502548 ] \n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "# code policy evaluation function\n",
    "def policyEval(itr, nA, nS, reward, prob, policy, valFunc, qFunc, gamma, convergence):\n",
    "    for _ in range(itr):\n",
    "        delta = 0\n",
    "        for s in range(nS):\n",
    "            prevVal = valFunc[s]\n",
    "            valFunc[s] = sum([(x[0] * (reward[x[1]] + gamma * valFunc[x[1]])) for x in prob[s][policy[s]]])\n",
    "            delta = max(delta, abs(prevVal - valFunc[s]))\n",
    "        if delta < convergence:\n",
    "            break\n",
    "    return valFunc, qFunc\n",
    "\n",
    "# code policy improvement function\n",
    "def policyImprove(itr, nA, nS, reward, prob, policy, valFunc, qFunc, gamma):\n",
    "    for s in range(nS):\n",
    "        policy[s] = np.argmax([(sum([(x[0] * (reward[x[1]] + gamma * valFunc[x[1]])) for x in prob[s][a]])) for a in range(nA)])\n",
    "    return policy, valFunc, qFunc\n",
    "\n",
    "# implement policy iteration\n",
    "def policyIteration(policyItr, valueItr, nA, nS, reward, prob, policy, valFunc, qFunc, gamma, convergence):\n",
    "    for i in range(policyItr):\n",
    "        policyPrev = policy.copy()\n",
    "        valFunc, qFunc = policyEval(valueItr, nA, nS, reward, prob, policy, valFunc, qFunc, gamma, convergence)\n",
    "        policy, valFunc, qFunc = policyImprove(valueItr, nA, nS, reward, prob, policy, valFunc, qFunc, gamma)\n",
    "        if ((policy == policyPrev).all()):\n",
    "            print(\"Terminated in\", i+1, \"policy iterations\")\n",
    "            return policy, valFunc\n",
    "            break\n",
    "    return policy, valFunc\n",
    "\n",
    "# Finding the optimal policy\n",
    "policy = np.zeros(num_states, dtype=np.int64)       # Initializing the policy with all LEFT\n",
    "valFunc = np.zeros(num_states)                      # Initializing the value function with all zeroes\n",
    "qFunc = np.zeros((num_states, num_actions))\n",
    "reward = [-0.01, -0.01, -0.01, 1, -0.01, 0, -0.01, -1, -0.01, -0.01, -0.01, -0.01]          # Rewards for the environment\n",
    "prob = env.P()                                  # The transition probability function for our environment                                                                  \n",
    "prob[5] = {0: [], 1: [], 2: [], 3: []}\n",
    "gamma = 0.9      \n",
    "convergence = 1e-5                               \n",
    "\n",
    "print(\"Optimal Policy:{} \\nValue Function:{} \".format(*policyIteration(1000, 1000, env.nA, env.nS, reward, prob, policy, valFunc, qFunc, gamma, convergence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Find an optimal policy to navigate the given environment using Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 111 value iterations\n",
      "Optimal Policy: [2 2 2 0 1 0 1 0 1 0 1 0]\n",
      "Value Function: [ 7.13970298  8.25558983  9.41609956  9.99991665  6.25681064  0.\n",
      "  6.3413181  -9.99991665  5.40786603  4.73617366  5.25652463  3.0502252 ]\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "def valueIteration(itr, nA, nS, reward, prob, policy, valFunc, gamma, convergence):\n",
    "    for _ in range(itr):\n",
    "        delta = 0\n",
    "        for s in range(nS):\n",
    "            prevVal = valFunc[s]\n",
    "            # Compute the new value for state s\n",
    "            new_val = compute_new_value(s, nA, reward, prob, valFunc, gamma)\n",
    "            valFunc[s] = new_val\n",
    "            # Update the maximum change in value\n",
    "            delta = max(delta, abs(prevVal - valFunc[s]))\n",
    "        # Check for convergence\n",
    "        if delta < convergence:\n",
    "            print(\"Converged in\", _+1, \"value iterations\")\n",
    "            break\n",
    "    \n",
    "    # Compute the optimal policy based on the updated value function\n",
    "    update_policy(nS, nA, reward, prob, valFunc, gamma, policy)\n",
    "    \n",
    "    return policy, valFunc\n",
    "\n",
    "def compute_new_value(s, nA, reward, prob, valFunc, gamma):\n",
    "    max_val = float('-inf')\n",
    "    # Calculate the maximum value for state s over all possible actions\n",
    "    for a in range(nA):\n",
    "        action_val = calculate_action_value(s, a, reward, prob, valFunc, gamma)\n",
    "        max_val = max(max_val, action_val)\n",
    "    return max_val\n",
    "\n",
    "def calculate_action_value(s, a, reward, prob, valFunc, gamma):\n",
    "    action_val = 0\n",
    "    for x in prob[s][a]:\n",
    "        action_val += x[0] * (reward[x[1]] + gamma * valFunc[x[1]])\n",
    "    return action_val\n",
    "\n",
    "def update_policy(nS, nA, reward, prob, valFunc, gamma, policy):\n",
    "    # Update policy based on the updated value function\n",
    "    for s in range(nS):\n",
    "        max_action_val = float('-inf')\n",
    "        best_action = None\n",
    "        # Find the action that maximizes the value for state s\n",
    "        for a in range(nA):\n",
    "            action_val = calculate_action_value(s, a, reward, prob, valFunc, gamma)\n",
    "            if action_val > max_action_val:\n",
    "                max_action_val = action_val\n",
    "                best_action = a\n",
    "        policy[s] = best_action\n",
    "\n",
    "# Finding the optimal policy\n",
    "policy = np.zeros(num_states, dtype=np.int64)       # Initializing the policy with action 0\n",
    "valFunc = np.zeros(num_states)                      # Initializing the value function as 0\n",
    "reward = [-0.01, -0.01, -0.01, 1, -0.01, 0, -0.01, -1, -0.01, -0.01, -0.01, -0.01]          # Rewards for the environment\n",
    "prob = env.P()                                  # The transition probability function for our environment                                                                  \n",
    "prob[5] = {0: [], 1: [], 2: [], 3:[]}\n",
    "gamma = 0.9                   \n",
    "convergence = 1e-5\n",
    "\n",
    "print(\"Optimal Policy: {}\\nValue Function: {}\".format(*valueIteration(1000, env.nA, env.nS, reward, prob, policy, valFunc, gamma, convergence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Compare PI and VI in terms of convergence. Is the policy obtained by both same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: Yes the policy obtained is same in this case by both PI and VI. Comapring the convergence of PI and VI, PI converges faster than VI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cafede8658e71bdc4b7180bcd658951c639327337cbd78715b7c29dc66075fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
